Spark Interview Coding Questions Series

I request you to participate in this series and comment down your answers below in the comment section. Before end of Day, I will post the solution as well in the comment section. So you have enough time till EOD to comment your ans.

Hope you will find this series interesting!!!

Question 1:- Consider we have two files- File 1 and File 2 with different schema, how will you merge both the file into single dataframe as shown:
File1:-
Roll_No|Subject|Marks
1001|English|84
1001|Pysics|55
1001|Maths|45
1001|Science|35
1001|History|32
1002|English|84
1002|Pysics|67
1002|Maths|90
1002|Science|89
1002|History|32

File2:
Reacharde_Id,Recaharge_Date,Reamaining_Days,validiy
123,20200511,2,online
124,20200612,67,online
125,20200713,89,online
126,20200814,78,online
127,20200916,20,online

Question 2:- From the given csv file, calculate expiry date as shown in output Dataframe by adding rechargedate column with remaining days column.
File:
Reacharde_Id,Recaharge_Date,Reamaining_Days,validiy
123,20200511,2,online
124,20200612,67,online
125,20200713,89,online
126,20200814,78,online
127,20200916,20,online

Stay tuned for more post regarding Big Data & SQL.

SPARK
-----
Department wise second highest salary using Spark Dataframes:
=========================================
Table: Department
columns: dept_is,dept_name,salary
val MyWindow = Window
.partitionBy("dept_id")
.orderBy("salary" desc)
val df2=df.withColumn("rownum", dense_rank() over MyWindow )
.filter("rownum=2")
df2.show();

File Format In Spark An Article from Linkedin
Parquet file format offers best compatibility with Apache Spark engine.

✅ Like ORC, parquet is a self-describing (schema embedded as part of data itself) columnar file format offering good 'read' performance and compression but the difference is that parquet can store deeply-nested data and queries on this nested data are made efficient by reading nested fields directly skipping all other fields in nested structure. Also ORC is optimized for Hive.
✅ Avro is a row-based file format which is good fit for storing data in landing zone of data lake and also offers best support for schema evolution.
✅ CSV format takes up huge storage space and there is type conversion overhead since everything is stored as string internally.
✅ JSON is similar to CSV in terms of performance and storage with the added disadvantage of not being splittable. If a file cannot be split, then it can't be a good fit for big data operations because it cannot leverage distributed storage and parallelism.

Q: What is differance in spark partitionBy() and bucketBy()?
Some pointers for your thought.
partitionBy() creates directories per column value.
bucketBy() creates files per bucket per RDD partition.

# here is the traditional way to define a shema in PySpark
import pyspark.sql.types as T

schema =
T.StructType([
T.StructField("col1",
T.StructField("col2",
T.StructField("col3",
])
schema
# output
StructType(List(StructField(col1,StringType,true),StructField(co12, IntegerType,
true), StructField(col3, TimestampType, true)))
# and here is the way using the helper function out of types
ddl_schema_string "coll string, col2 integer, col3 timestamp"
T._parse_datatype_string(ddl_schema_string)
ddl_schema
ddl_schema
T.StringType(), True),
T.IntegerType(), True),
T.TimestampType(), True)
# output
=
StructType(List(StructField(coll,StringType,true),StructField(col2, IntegerType,
true),StructField(col3,TimestampType,true)))

If you partitionBy("col1"), you will see one directory for each unique value of col1.
If you bucketBy(3, "col1"), you will see 3 x N bucket files. The N is your dataframe RDD partitions.
If you select * from part_tbl where col1='XYZ', the partition pruning will take place.
If you select * from bkt_tbl where col1='XYZ', the bucket pruning will not happen.
So bucketBy() in Spark is not a solution for optimized record filtering.
Then why do we have bucketBy() in Spark?
They are an excellent tool to eliminate exchange and avoid shuffling for the group by and join queries.
There are a few more associated concepts, such as full shuffle, partial shuffle, bucket skew, small file problems, etc.
Efficient use of buckets in Spark takes a lot of in-depth understanding, or you have many pitfalls and traps.
But when used correctly, it can boost your joins and aggregations beyond imagination.
Add your thoughts in the comment.

PS: I teach mastering Spark and Databricks Cloud. Next batch starting 14th August. For course inquiries, fill out the form (Link in comments)
